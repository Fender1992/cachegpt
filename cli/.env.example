# LLM Cache CLI Configuration
# Copy this to .env and update with your values

# Base URL of your LLM Cache Proxy server
LLM_CACHE_BASE_URL=http://localhost:8000

# Your API key from the LLM Cache Proxy
LLM_CACHE_API_KEY=sk-your-api-key-here

# Default LLM model to use for testing
LLM_CACHE_DEFAULT_MODEL=gpt-3.5-turbo

# Request timeout in seconds
LLM_CACHE_TIMEOUT=30